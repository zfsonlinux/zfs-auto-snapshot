#!/usr/bin/ksh93

#
# CDDL HEADER START
#
# The contents of this file are subject to the terms of the
# Common Development and Distribution License, Version 1.0 only
# (the "License").  You may not use this file except in compliance
# with the License.
#
# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
# or http://www.opensolaris.org/os/licensing.
# See the License for the specific language governing permissions
# and limitations under the License.
#
# When distributing Covered Code, include this CDDL HEADER in each
# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
# If applicable, add the following below this CDDL HEADER, with the
# fields enclosed by brackets "[]" replaced with your own identifying
# information: Portions Copyright [yyyy] [name of copyright owner]
#
# CDDL HEADER END
#
# Copyright 2009 Sun Microsystems, Inc.  All rights reserved.
# Use is subject to license terms.
#

# zfs-auto-snapshot changeset = ~ZFS_AUTO_SNAPSHOT_CHANGESET~

# This SMF method takes periodic snapshots of zfs filesystems, with
# options to allow the user to keep a limited number of snapshots, snapshot
# all child datasets, or run zfs send piping to a specified command.
# More documentation is available at
# http://src.opensolaris.org/source/xref/jds/zfs-snapshot/README.zfs-auto-snapshot.txt
#
# The service will move itself into maintenance if it's unable to take a
# snapshot, destroy a snapshot as per the snapshot retention policy, is unable to
# zfs send a dataset (if configured) or is unable to create or update the cron
# job.
#



# For interested developers, the main functions here, are schedule_snapshots,
# unschedule_snapshots and take_snapshot : the exit conditions from these
# functions check the state of the service before returning an appropriate
# value. The check_failure method is responsible for checking error codes from
# subprocesses, and when called with a non-zero argument, will degrade the
# service, and log an appropriate error message.

. /lib/svc/share/smf_include.sh

result=$SMF_EXIT_OK

export PATH=/usr/sbin:/usr/bin:${PATH}

# A prefix we use on all snapshots created by this script.
# See the definition of $SNAPNAME in the take_snapshot()
# function for more information.
PREFIX="zfs-auto-snap"

# A flag we use to determine whether to echo log messages
# to stdout (and get picked up by SMF) or remain silent
# and use logger(1) to log to syslog instead if the flag is
# null.
LOG=""

# OpenSolaris has no pfksh, so for now, we need to pfexec
# all calls to zfs that require privileges
function pfexec_zfs {
	pfexec zfs $@
	return $?
}

function get_pair {
        NAME=$1
        shift 2
        echo "${NAME}=\"$@\""
        echo "export ${NAME}"
}

# A function to pull in the variables from the FMRI given
# as the first argument.
#
function zfs_smf_props {

IFS="
"
SMF_PROPS="$(svcprop -t -p zfs $1 |\
            sed -e 's#zfs/fs-name#zfs/fs_name#g' \
        -e 's#zfs/backup-lock#zfs/backup_lock#g' \
        -e 's#zfs/snapshot-children#zfs/snapshot_children#g' \
        -e 's#zfs/backup-save-cmd#zfs/backup_save_cmd#g' \
	-e 's#zfs/auto-include#zfs/auto_include#g' \
        -e 's#zfs/##g' -e 's/$/,/g')"
IFS=,
for line in $SMF_PROPS ; do
        IFS=' 	
'
        eval $(get_pair $line)
done
}


# this function validates the properties in the FMRI passed to it, then
# calls a function to create cron job that schedules a snapshot schedule based
# on the properties set in the service instance.
# $1 is assumed to be a valid FMRI
#
function schedule_snapshots {

	typeset FMRI=$1
	zfs_smf_props $FMRI 

	# FIXME need work in here to actually validate the FMRI props
	typeset FILESYS="$fs_name"
	typeset INTERVAL="$interval"
	typeset PERIOD="$period"
	typeset OFFSET="$offset"
	typeset STATE=0	

	typeset BACKUP="$backup"
	typeset BACKUP_SAVE_CMD="$backup_save_cmd"

	case $BACKUP in
	'full' | 'incremental' )
	    if [ -z "${BACKUP_SAVE_CMD}" ] ; then
	        check_failure 1 \
		    "Backup requested, but no backup command specified."
	    fi
	;;
	esac

	case $FILESYS in
	'//')
		;;
	*)
		# validate the filesystem
		zfs list $FILESYS 2>&1 > /dev/null
		check_failure $? "ZFS filesystem does not exist!"
		;;
	esac

	# remove anything that's there at the moment
	unschedule_snapshots $FMRI		
	check_missed_snapshots $INTERVAL $PERIOD $FMRI

	add_cron_job $INTERVAL $PERIOD $OFFSET $FMRI
	
	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi	
	return $STATE
}


# Adding a cron job that runs exactly every x time-intervals is hard to do
# properly.
#
# For now, what we're doing, is dividing the interval up into x bite chunks
# and running the cron job that often. The problem comes where the interval
# doesn't evenly divide up into x, leaving us taking to many, or too
# few snapshots at the edge of time intervals.
#
# A new implementation of cron would be nice, but until then, we'll
# just live with this.
#
function add_cron_job { # $INTERVAL $PERIOD $OFFSET $FMRI

	typeset INTERVAL=$1
	typeset PERIOD=$2
	typeset OFFSET=$3
	typeset FMRI=$4

	DAYS=$((${OFFSET} / 86400))
	LEFT=$((${OFFSET} % 86400))
	HOURS=$((${LEFT} / 3600))
	LEFT=$((${LEFT} % 3600))
	MINUTES=$((${LEFT} / 60))

	((${DAYS} > 31)) && DAYS=31

	case $INTERVAL in
	'minutes')
	    ((${PERIOD} < ${MINUTES})) && MINUTES=0
	    TIMES=$(get_divisor ${MINUTES} 59 $PERIOD)
	    ENTRY="$TIMES * * * *"		
	;;

	'hours')
	    ((${PERIOD} < ${HOURS})) && HOURS=0
	    TIMES=$(get_divisor ${HOURS} 23 $PERIOD)
	    ENTRY="${MINUTES} $TIMES * * *"
	;;

	'days')
	    ((${PERIOD} < ${DAYS} || ${DAYS} == 0)) && DAYS=1
	    TIMES=$(get_divisor ${DAYS} 31 $PERIOD)
	    ENTRY="${MINUTES} ${HOURS} $TIMES * *"
	;;

	'months')
	    ((${DAYS} == 0)) && DAYS=1
	    TIMES=$(get_divisor 1 12 $PERIOD)
	    ENTRY="${MINUTES} ${HOURS} ${DAYS} $TIMES *"
	;;

	'none')
	    return 0
	;;
	esac

	# Since we may have multiple instances all trying to start at
	# the same time, we need some form of locking around crontab.
 	# Normally we'd be able to get SMF to manage this, by defining dependencies -
	# but we're not sure there's a way to prevent it from starting two instances
	# at the same time (without requiring users to explicitly state dependencies
	# and change them each time new instances are added)

	# This isn't perfect (eg. if someone else is running crontab at the
	# same time as us, we'll fail) but it'll do for now.
	LOCK_OWNED="false"
	while [ "$LOCK_OWNED" == "false" ] ; do
	    mkdir /tmp/zfs-auto-snapshot-lock > /dev/null 2>&1
	    if [ $? -eq 0 ] ; then
	        LOCK_OWNED=true
	    else
	        sleep 0.1
	    fi
	done

	# adding a cron job is essentially just looking for an existing entry,
	# removing it, and appending a new one.
	crontab -l | grep -v "/lib/svc/method/zfs-auto-snapshot $FMRI$" \
	    > /tmp/saved-crontab.$$

	echo "${ENTRY} /lib/svc/method/zfs-auto-snapshot $FMRI" \
	    >> /tmp/saved-crontab.$$

	crontab /tmp/saved-crontab.$$
	check_failure $? "Unable to add cron job!"
	
	# release our lock
	rm -rf /tmp/zfs-auto-snapshot-lock
	rm /tmp/saved-crontab.$$
	return 0
}



# this function removes a cron job was taking snapshots of $FMRI
# $1 is assumed to be a valid FMRI
#
function unschedule_snapshots {

	typeset FMRI=$1
	typeset INTERVAL=$interval

	# interval set to 'none' means we don't want cron jobs
	if [ "$INTERVAL" == "none" ]; then
	    return 0
	fi

	# See notes on $LOCK_OWNED variable in function add_cron_job
        LOCK_OWNED="false"
        while [ "$LOCK_OWNED" == "false" ]; do
	    mkdir /tmp/zfs-auto-snapshot-lock > /dev/null 2>&1
	    if [ $? -eq 0 ] ; then
	        LOCK_OWNED=true
	    else
	        sleep 0.1
	    fi
	done

	crontab -l | grep -v "/lib/svc/method/zfs-auto-snapshot $FMRI$" \
	    > /tmp/saved-crontab.$$

	crontab /tmp/saved-crontab.$$
	check_failure $? "Unable to unschedule snapshots for $FMRI"

	rm -rf /tmp/zfs-auto-snapshot-lock
	rm /tmp/saved-crontab.$$

	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi
}


# This function is intended to be called on service start. It checks to see
# if the last snapshot was taken more than <frequency> <intervals> ago,
# and if that's the case, takes a snapshot immediatedly.
#
function check_missed_snapshots { # $INTERVAL $PERIOD $FMRI <repopulate cache>

	typeset INTERVAL=$1
	typeset PERIOD=$2
	typeset FMRI=$3
	typeset FILESYS=$fs_name
	typeset LABEL=$label
	typeset SEP=$sep
	
	IFS="	
"

	if [ -n $4 ] ; then
		typeset NO_CACHE_REPOPULATE=$4
	fi

	# // is special, in that we take snapshots based on user properties
	# so here, we get those properties and call ourselves again, with
	# those values.
	case "$FILESYS" in
	"//")
		get_userprop_datasets	
		export snapshot_children=false
		export fs_name=${SINGLE_LIST}
		echo "Checking for non-recursive missed // snapshots $SINGLE_LIST"
		check_missed_snapshots $INTERVAL $PERIOD $FMRI no_repopulate_cache

		export snapshot_children=true
		export fs_name=${RECURSIVE_LIST}
		echo "Checking for recursive missed // snapshots $RECURSIVE_LIST"
		check_missed_snapshots $INTERVAL $PERIOD $FMRI no_repopulate_cache

		return 0
		;;
	esac

	
	if [ "$LABEL" != "\"\"" ] ; then
		LABEL="${SEP}${LABEL}"
	else
		LABEL=""
	fi

	# check to see if there are any filesystems
	if [ -z "$FILESYS" ] ; then
		return 0
	fi

	# only interested in the first filesystem, assuming they
	# all have a similar creation date.
	set -A fs $FILESYS
	LAST_SNAPSHOT=$(zfs list -s creation -H -o name -r -t snapshot ${fs[0]} \
		| egrep "${fs[0]}@${PREFIX}${LABEL}"\|"${fs[0]}@${PREFIX}:${label}" | tail -1)

	# if we've never taken a snapshot, then we should
	# arrange to have one taken now. Fake the LAST_SNAP_TIME variable
	if [ -z "$LAST_SNAPSHOT" ] ; then
		LAST_SNAP_TIME=0
		LAST_SNAP_TIME_HUMAN="[ no snapshot taken yet ]"
	fi

	# if we have taken a snapshot, find out when it was
	if [ -n "$LAST_SNAPSHOT" ] ; then
		LAST_SNAP_TIME=$(zfs get -H -p -o value creation $LAST_SNAPSHOT)
		LAST_SNAP_TIME_HUMAN=$(zfs get -H -o value creation $LAST_SNAPSHOT)
	fi
	NOW=$(perl -e 'print time;')

	# slightly incorrect time accounting here, but good enough.
	MINUTE_S=60
	HOUR_S=$(( $MINUTE_S * 60 ))
	DAY_S=$(( $HOUR_S * 24 ))
	MONTH_S=$(( $DAY_S * 30 ))
	
	case $INTERVAL in
	"minutes")
		MULTIPLIER=$MINUTE_S
	;;
	"hours")
		MULTIPLIER=$HOUR_S
	;;
	"days")
		MULTIPLIER=$DAY_S
	;;
	"months")
		MULTIPLIER=$MONTH_S
	;;
	"none")
		return 0
	;;
	"*")
		print_log "WARNING - unknown interval encountered in check_missed_snapshots!"
		return 1
	esac
	PERIOD_S=$(( $MULTIPLIER * $PERIOD ))
	AGO=$(( $NOW - $LAST_SNAP_TIME ))
	if [ $AGO -gt $PERIOD_S ] ; then
		echo "Last snapshot for $FMRI taken on $LAST_SNAP_TIME_HUMAN"
		echo "which was greater than the $PERIOD $INTERVAL schedule. Taking snapshot now."
		take_snapshot $FMRI $NO_CACHE_REPOPULATE
	fi
}

# This function actually takes the snapshot of the filesystem.
# $1 is assumed to be a valid FMRI. $2 if non-null makes us skip
# populating the SMF property cache - used only by the special
# // snapshot type.
function take_snapshot {
	IFS="	
"

	# want this to be global, used by check_failure
	FMRI=$1
	NO_CACHE_REPOPULATE=$2

	if [ -z "$NO_CACHE_REPOPULATE" ] ; then
		zfs_smf_props $FMRI
	fi

	typeset SEP=$sep
	typeset DATE=$(date +%F-%H%M)
	typeset FILESYS="$fs_name"
	typeset KEEP=$keep
	typeset SNAP_CHILDREN=$snapshot_children

	typeset BACKUP=$backup

	typeset STATE=0

	# an identifier allows us to setup multiple snapshot schedules
	# per filesystem - so we append a <sep><label> token if the user has
	# requested one, which then gets used in the SNAPNAME.  SMF
	# returns the value '""' for the empty string to differentiate
	# between an unset property, and a set-but-empty property.
	typeset LABEL="$label"
	
	# the "//" filesystem is special. We use it as a keyword
	# to determine whether to poll the ZFS "com.sun:auto-snapshot:${LABEL}"
	# user property which specifies which datasets should be snapshotted
	# and under which "label" - a set of default service instances that
	# snapshot at defined periods (daily, hourly, weekly, monthly, every
	# 15 mins) are provided already.
	# Determine what these are, call ourselves again, then return.
	case "$FILESYS" in
	"//")
		# this populates two values SINGLE_LIST and RECURSIVE_LIST
		get_userprop_datasets $LABEL

		print_note "Taking non-recursive snapshots $SINGLE_LIST"
		export snapshot_children=false
		export fs_name=${SINGLE_LIST}
		take_snapshot $FMRI no_propcache_repopulate
		single_STATE=$?

		print_note "Taking recursive snapshots of $RECURSIVE_LIST"
		export snapshot_children=true
		export fs_name=${RECURSIVE_LIST}
		take_snapshot $FMRI no_propcache_repopulate
		recursive_STATE=$?
		return $single_STATE && $recursive_STATE
	;;
	esac	
		
	if [ "$LABEL" != "\"\"" ] ; then
	    LABEL="${SEP}${LABEL}"
	else
	    LABEL=""
	fi

	# A flag for whether we're running in verbose mode
	VERBOSE="$verbose"

	typeset SNAPNAME="${PREFIX}${LABEL}-${DATE}"
	
	# Determine whether we should avoid pools which are scrubbing
	typeset AVOIDSCRUB=$avoidscrub

	# prune out the filesystems that are on pools currently being
	# scrubbed or resilvered. There's a risk that a scrub/resilver
	# will be started just after this check completes, and also
	# the risk that a running scrub will complete just after this
	# check.
	if [ "$AVOIDSCRUB" == "true" ] ; then
	    # lists of pools and datasets that are known not to be scrubbing
	    NOSCRUBLIST=""
	    NOSCRUBFILESYS=""

	    # Create a list of filesystems scheduled for snapshots
	    # that are *not* on pools that are being scrubbed/resilvered
	    for fs in $FILESYS ; do
		POOL=$(echo $fs | cut -d/ -f1)
		if is_scrubbing ${POOL} ${NOSCRUBLIST} ; then
		    print_log "Pool containing $fs is being scrubbed/resilvered."
		    print_log "Not taking snapshots for $fs."
		else
		    NOSCRUBLIST="${POOL}	${NOSCRUBLIST}"
		    NOSCRUBFILESYS="${NOSCRUBFILESYS}	${fs}"
		fi
	    done		
	    FILESYS="$NOSCRUBFILESYS"
	fi
	# walk each of the filesystems specified
	for fs in ${FILESYS} ; do
	    # Ok, now say cheese! If we're taking recursive snapshots,
	    # walk through the children, destroying old ones if required.
	    if [ "${SNAP_CHILDREN}" == "true" ] ;  then

	         destroy_older_snapshots $fs $KEEP $LABEL "do_recursive_destroy"
	         print_note "Taking recursive snapshot $fs@$SNAPNAME"
	         pfexec_zfs snapshot -r "$fs@$SNAPNAME"
	         check_failure $? "Unable to take recursive snapshots of $fs@$SNAPNAME."
	
	    else
	         destroy_older_snapshots $fs $KEEP $LABEL ""
		 print_note "Taking snapshot $fs@$SNAPNAME"
		 pfexec_zfs snapshot "$fs@$SNAPNAME"
		 check_failure $? "Unable to take snapshot $fs@$SNAPNAME."
	    fi

	    # If the user has asked for backups, go ahead and do this.
	    if [ "${BACKUP}" != "none" ] ;  then
	        take_backup "$fs" $BACKUP "$LABEL" $FMRI
	        check_failure $? "Unable to backup filesystem $fs using \
		        $BACKUP backup strategy."
	    fi

	done
	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi
	return $STATE
}

# Given a filesystem name, and a limit of the number of snapshots we want,
# along with the identifier for this set of snapshots,
# we destroy all older snapshots of this filesystem whose names begin
# with the text "${PREFIX}${LABEL}". Note that here we destroy one more snapshot
# than the "keep" threshold - this is because in the context of calling this
# function, we're already creating one new auto-snapshot.
#
function destroy_older_snapshots {

	typeset FILESYS=$1
	typeset COUNTER=$2
	typeset LABEL=$3
	typeset DO_RECURSIVE=$4

	IFS="	
"

	if [ "${COUNTER}" == "all" ] ; then
	    return 0
	fi

	if [ -n "${DO_RECURSIVE}" ] ; then
	    typeset RECURSIVE="-r"
	fi
	
	COUNTER=$(($COUNTER - 1))
	# walk through the snapshots, newest first, destroying older ones
	for snapshot in $(zfs list -S creation -r -t snapshot -H -o name $FILESYS \
		 | egrep -e "$FILESYS@${PREFIX}${LABEL}"\|"$FILESYS@${PREFIX}:${label}") ; do
		echo $COUNTER
	    if [ $COUNTER -le 0 ] ; then
	        print_note "$snapshot being destroyed ${RECURSIVE} as per \
 retention policy."
	        pfexec_zfs destroy ${RECURSIVE} "$snapshot"
	        check_failure $? "Unable to destroy $snapshot" "NON_FATAL"
	    else
	        # don't destroy this one			
	        COUNTER=$(($COUNTER - 1))
	    fi
	done
}

# Given the exit status of a command, an integer, 0 if the command completed
# without errors. If the command exited with errors we degrade the
# state of this service into maintenance mode. If a 3rd argument is presented
# we don't degrade the service. We also log an error message as passed into
# this function.
#
function check_failure { # integer exit status, error message, non-fatal flag

	typeset RESULT=$1
	typeset ERR_MSG=$2
	typeset NON_FATAL=$3

	if [ $RESULT -ne 0 ] ; then
	    print_log "Error: $ERR_MSG"
	    if [ -z "${NON_FATAL}" ] ; then
		print_log "Moving service $FMRI to maintenance mode."
	        svcadm mark maintenance $FMRI
	    fi
	fi

}


# A function we use to emit output. Right now, this goes to syslog via logger(1)
# as well as being echoed to stdout which will result in it being picked up by
# SMF if the $LOG variable is null.
#
function print_log { # message to display
	logger -t zfs-auto-snap -p daemon.notice $*
	if [ -z "$LOG" ] ; then
		echo $*
	fi
}

# Another function to emit output, this time checking to see if the
# user has set the service into verbose mode, otherwise, we print nothing
#
function print_note { # mesage to display
	if [ "$VERBOSE" == "true" ] ; then
		if [ -z "$LOG" ] ; then
			echo $*
		else
			print_log $*
		fi
	fi
}


# Given a range start, end and width of period, return a comma
# separated string of numbers within that range and conforming to
# that period. This isn't ideal, but it'll do for now.
#
function get_divisor { # start period, end period, width of period

	typeset START=$1
	typeset END=$2
	typeset WIDTH=$3
	typeset RANGE=$START
	typeset JUMP=$(( $RANGE + $WIDTH ))

	while [ $JUMP -le $END ] ; do
	    RANGE="$RANGE,$JUMP"
	    JUMP=$(( $JUMP + $WIDTH ))
	done
	
	echo $RANGE
}


# Given a filesytem name, and a backup type (currently "complete" or
# "incremental") along with an FMRI, we backup the filesystem - either
# from the latest snapshot that was taken, or by an incremental backup.
# Properties in the FMRI tell us what to do with the backup stream
#
function take_backup { # filesystem backup-type label fmri

    typeset FILESYS=$1
    typeset BACKUP=$2
    typeset LABEL=$3
    typeset FMRI=$4

    IFS="	
"

    # obtain lock from fmri
    typeset LOCK=$(svcprop -p zfs/backup-lock $FMRI)
    if [ "$LOCK" != "unlocked" ]
    then
	# Unable to perform this backup due to an existing backup being
	# executed for this dataset. This would result in moving the
	# service to maintenance mode if we're doing incrementals, since
	# missing an incremental backup will result in the user being unable
	# to restore future incremental backups. This isn't so serious for
	# full backups.
        print_log "Unable to backup $FILESYS: $LOCK."

	if [ "$BACKUP" == "incremental" ]
	then
	     print_log "A lock prevented us from performing an incremental backup."
	     return 1
	else
	     print_log "Full backup not completed for $FMRI."
	     return 0
	fi
    else
 	# set our lock. (this isn't atomic, unfortunately :-( )
        svccfg -s $FMRI setprop zfs/backup-lock = astring: \
	"\"$BACKUP backup in progress by PID $$\""
	svcadm refresh $FMRI
    fi

    typeset BACKUP_SAVE_CMD="$(echo $backup_save_cmd | sed -e 's/\\//g')"
    typeset SNAP_CHILDREN=$snapshot_children
    typeset BACKUP_DATASETS=""

    # Determine how many datasets we have to backup
    if [ "$SNAP_CHILDREN" == "true" ] ; then
	BACKUP_DATASETS=$(zfs list -r -H -o name -t filesystem,volume $FILESYS)
    else
        # only one dataset to worry about here.
	BACKUP_DATASETS=$FILESYS
    fi

    # loop through the datasets, backing up each one.
    for dataset in $BACKUP_DATASETS ; do

      # An initial check of the input parameters, to see how we should proceed
      case $BACKUP in
        "incremental")
		# get the last two snapshots
		LAST_SNAP=$(zfs list -s creation -H -o name -r -t snapshot $dataset \
			    | egrep -e "$dataset@${PREFIX}${LABEL}"\|"$dataset@${PREFIX}:${label}" \
			    | tail -1)

		PREV_SNAP=$(zfs list -s creation -H -o name -r -t snapshot $dataset \
			    | egrep -e "$dataset@${PREFIX}${LABEL}"\|"$dataset@${PREFIX}:${label}" \
			    | tail -2 | head -1)

		if [ "$PREV_SNAP" == "$LAST_SNAP" ]
		then
		    print_log "Previous snap not found of $dataset, taking full backup."
		    BACKUP="full"
		fi
	;;
        "full")
 		LAST_SNAP=$(zfs list -s creation -H -o name -r -t snapshot $dataset \
			    | egrep -e "$dataset@${PREFIX}${LABEL}"\|"$dataset@${PREFIX}:${label}" \
			    | tail -1)
	;;
        *)
		check_failure 1 "Unknown backup type $BACKUP"
		svccfg -s $FMRI setprop zfs/backup-lock = astring: "unlocked"
		svcadm refresh $FMRI
		return 1
	;;
       esac


       # Now perform the backup. Note that on errors, we'll immediately mark
       # the service as being in maintenance mode, however, backups will still
       # be attempted for other datasets in our list.
       case $BACKUP in
	"incremental")
	    print_note "Starting incr. ZFS send of differences between $PREV_SNAP and $LAST_SNAP."
	    export LAST_SNAP
	    export PREV_SNAP
	    pfexec_zfs send -i "$PREV_SNAP" "$LAST_SNAP" | $BACKUP_SAVE_CMD
	    check_failure $? "Error performing incremental backup of $dataset."
	;;
	"full")
	    print_note "Starting ZFS send of $LAST_SNAP."
	    export LAST_SNAP
	    pfexec_zfs send "$LAST_SNAP" | $BACKUP_SAVE_CMD
	    check_failure $? "Error performing full backup of $dataset."
	;;
       esac
   done
   print_note "Backups completed for $dataset."
   # Now we can release our lock
   svccfg -s $FMRI setprop zfs/backup-lock = astring: "unlocked"
   svcadm refresh $FMRI

}


# Given a sorted list of filesystems of which we can take recursive snapshots,
# determine whether any of the listed filesystems are redundant
# eg. for tank/other tank/foo tank/other/bar tank/foo/bar
# we only need to recursively snapshot tank/other and tank/foo
function narrow_recursive_filesystems {

	IFS="	
"
	# for each filesystem in the list, walk back through each of it's
	# ancestors. If any of the ancestors is already in the list, don't
	# add it to the list of recursive snapshots, otherwise, do.
	typeset LIST=""
	for ds in $@ ; do
		ANCESTOR_IN_LIST=""
		last="$ds"
		# the equivalent of dirname $ds
		ancestor=${ds%/*}
		if [ "$last" == "$ancestor" ] ; then
			ancestor="."
		fi

		while [ $ancestor != "." ] ; do
			# check to delete "$ancestor//" from the list
			# if it wasn't there, we get back the same list
			NEW=${LIST##*$ancestor//*}
			if [ "$LIST" != "$NEW" ] ; then
				ANCESTOR_IN_LIST=true
			fi
			# replace ancestor with the equivalent of
			# /usr/bin/dirname $ancestor
			last=${ancestor}
			ancestor=${ancestor%/*}
			# do what dirname does when we reach the last entry
			if [ $last == "$ancestor" ] ; then
				ancestor="."
			fi
		done
		if [ -z "${ANCESTOR_IN_LIST}" ] ; then
			if [ -z "$LIST" ] ; then
				LIST="${ds}//"
			else
				LIST="${LIST}\t${ds}//"
			fi
		fi
	done
	# remove the // separators from the string
	print -n ${LIST//\/\//}
}

function can_recursive_snapshot {
        typeset ds=$1
        if egrep "$ds/"\|"$ds	" $EXCLUDE > /dev/null; then
                # we can't recursively snapshot $ds because
                # it's excluded or is in the path to an excluded dataset
                return 1
        else
                return 0
        fi
}

function is_excluded {
        typeset ds=$1
	# try to delete "$ds	" in exclude, if we get back the
	# same list, then it wasn't there

	NEW=${EXCLUDE_VAR##*$ds	*}
	if [ "$NEW" == "$EXCLUDE_VAR" ] ; then
		return 1
	else
		return 0
	fi
}

# This builds two lists of datasets - RECURSIVE_LIST and SINGLE_LIST
# based on the value of ZFS user properties com.sun:auto-snapshot and
# com.sun:auto-snapshot:${LABEL}, the first argument to this script.
# RECURSIVE_LIST is a list of datasets that can be snapshotted with -r
# SINGLE_LIST is a list of datasets to snapshot individually.
#
function get_userprop_datasets {

	typeset LABEL=$1
	export ALL=/tmp/zfs-auto-snapshot-list.$$
	export EXCLUDE=/tmp/zfs-auto-snapshot-exclude.$$

	IFS="	
"
  	zfs list -H -t filesystem,volume -s name -o \
	name,com.sun:auto-snapshot,com.sun:auto-snapshot:${LABEL} > $ALL
	cat $ALL | egrep -e "false$"\|"false	-$" > $EXCLUDE
	# save the above in a variable, preserving newlines
	IFS=''
	EXCLUDE_VAR=$(< $EXCLUDE)
	export EXCLUDE_VAR
	IFS="
"

	# iterating through datasets
	for ds in $( cut -f1 < $ALL ) ; do
        	if can_recursive_snapshot ${ds} ; then
                	print_note "OK to recursive snapshot $ds"
			if [ -z "$RECURSIVE_LIST" ] ; then
				RECURSIVE_LIST="$ds"
			else
                		RECURSIVE_LIST="${RECURSIVE_LIST}	$ds"
			fi
        	else
                	if ! is_excluded $ds ; then
                        	print_note "OK to snapshot sole dataset $ds"
				if [ -z "$SINGLE_LIST" ] ; then
					SINGLE_LIST="$ds"
				else
                        		SINGLE_LIST="${SINGLE_LIST}	$ds"
				fi
                	else
                        	print_note "$ds will not be snapshotted"
                	fi
        	fi
	done

	# return immediately if no datasets were selected
	if [ -z "${RECURSIVE_LIST}" ] ; then
		return
	fi
	print_note "Pre-Narrowed recursive list is $RECURSIVE_LIST"
	FINAL_RECURSIVE_LIST=$(narrow_recursive_filesystems $RECURSIVE_LIST)
	print_note "Narrowed list of datasets to recursively snapshot is"
	print_note "$FINAL_RECURSIVE_LIST"
	export RECURSIVE_LIST="$FINAL_RECURSIVE_LIST"
	export SINGLE_LIST

	rm $ALL
	rm $EXCLUDE
}

# Determine if a pool is currently being scrubbed or resilvered.
# Return 0 if it is scrubbing/resilvering, 1 otherwise.

# The 2nd arg is a cache of pools known to be not scrubbing during this
# invocation of the script. This does risk a scrub starting mid-way through
# the script and us not checking for it - but if that's just
# happened, then restarting the scrub as a result of a snapshot being taken
# won't be too expensive.
function is_scrubbing { # POOL SCRUBLIST
	
	typeset POOL=$1
	typeset NOSCRUBLIST="$2"
	typeset SCRUBBING=""

	# see if we can avoid running zpool status, by checking for
	# the pool name in a known list of pools that were not scrubbing
	# the last time we checked.
	echo "$NOSCRUBLIST" | grep "$POOL	" > /dev/null
	if [ $? -eq 0 ] ; then
	    return 1
	fi

	SCRUBBING=$(env LC_ALL=C zpool status $POOL | grep " in progress")
	if [ -z "$SCRUBBING" ] ; then
	    return 1
	else
	    return 0
	fi
}

# This function runs on startup - by default, if we're taking snapshots 
# under a // schedule, and there isn't a property set on the pool
# com.sun:auto-snapshot=false, then we set the property to true, causing
# all datasets on the system to get included by the service. 
function auto_include {
	FS_NAME=$fs_name
	LABEL=$label
	IFS="	
"
	if [[ "$FS_NAME" == "//" && "$auto_include" == "true" ]] ; then
		POOLS=$(zpool list -H -o name)
		for pool in $POOLS ; do
			if ! zpool status -x $pool | grep "state: UNAVAIL" > /dev/null ; then
				SNAPALL=$(zfs get -H -o value com.sun:auto-snapshot $pool)
				SNAPLABEL=$(zfs get -H -o value com.sun:auto-snapshot:$LABEL $pool)
				SNAP=$SNAPALL$SNAPLABEL
				case $SNAP in
				*true | true*)
					;;
				*false | false*)
					;;
				*)
					FIRSTRUN_POOLS="$pool	$FIRSTRUN_POOLS"
				;;
				esac
			fi
		done	
	fi
	
	for pool in ${FIRSTRUN_POOLS} ; do
		# This is the first time the service has run on this pool,
		# set appropriate zfs user properties on it
		$PFZFS set com.sun:auto-snapshot=true $pool
	done

	# check for swap devices using zvols. 
	# XXX spaces in dataset names cause problems here
	SWAP=$(swap -l | grep "/dev/zvol/dsk/" | awk '{print $1}')
	SWAPVOLS=""
	for swap in $SWAP ; do
		SWAPVOLS="$SWAPVOLS $(echo $swap | sed -e 's#/dev/zvol/dsk/##')"
	done

	# can't run dumpadm as a non-root user, so check for dumpadm.conf
	if [ -r /etc/dumpadm.conf ] ; then
		DUMP=$(grep "^DUMPADM_DEVICE=" /etc/dumpadm.conf |\
			grep "/dev/zvol/dsk/" |\
			awk -F= '{print $2}')
		if [ -n "$DUMP" ] ; then
			DUMPVOL=$(echo $DUMP | sed -e 's#/dev/zvol/dsk/##')
		fi
	fi

	if [ -n "${SWAPVOLS}${DUMPVOL}" ] ; then
		# disable snapshots on our swap and dump devices
		pfexec_zfs set com.sun:auto-snapshot=false $SWAPVOLS $DUMPVOL
	fi
}



# Here's the beginning of the main script. As we're a method script for SMF,
# we take start and stop arguments, and assume that the $SMF_FMRI value is being
# set. For start and stop, our task is to create a cron job that will take a
# snapshot of the specified filesystem.
#
# Without start | stop arguments, we assume we're being called from the cron job
# created above, where the argument is the FMRI containing properties we can
# consult to in order to actually take the snapshot.


# When called via SMF, several variables are set for us - SMF_FMRI being one
# we check for the existance of that
if [ -n "${SMF_FMRI}" ] ; then
	zfs_smf_props $SMF_FMRI
fi

# $1 start | stop | refresh | an FMRI that we want to take snapshots of.
case "$1" in
'start')
	auto_include $SMF_FMRI
	schedule_snapshots $SMF_FMRI
	if [ $? -eq 0 ] ; then
	    result=$SMF_EXIT_OK
	else
	    print_log "Problem taking snapshots for $SMF_FMRI"
	    result=$SMF_EXIT_ERR_FATAL
	fi
        ;;

'stop')
	unschedule_snapshots $SMF_FMRI
	if [ $? -eq 0 ] ; then
	    result=$SMF_EXIT_OK
	else
	    print_log "Problem taking snapshots for $SMF_FMRI"
	    result=$SMF_EXIT_ERR_FATAL
	fi
        ;;
# the default case, we actually call from the cron job itself that's
# executing this script, and do the job of taking snapshots.
*)
	SMF_FMRI=$1
	# are we being called with the correct argument (an FMRI) ?

	case $SMF_FMRI in
		svc:/*)
			TTYOUTPUT=$(zfs_smf_props $SMF_FMRI 2>&1)
			export LOG="true"
			if [ -n "$TTYOUTPUT" ] ; then
			    print_log "$TTYOUTPUT"
			fi

			TTYOUTPUT=$(take_snapshot $SMF_FMRI 2>&1)
			if [ $? -eq 0 ] ; then
			    result=$SMF_EXIT_OK
			else
			    result=$SMF_EXIT_ERR_FATAL
			fi

			if [ -n "$TTYOUTPUT" ] ; then
			    print_log "$TTYOUTPUT"
			fi
			;;
		*)
			# not logging these messages - we're assuming a curious
			# user has run the script from the command line.
			echo "Usage from SMF : zfs-auto-snapshot [start | stop]"
			echo "Usage from cron: zfs-auto-snapshot svc:/system/filesystem/zfs/auto-snapshot:instance"
			result=$SMF_EXIT_ERR_FATAL
			;;
		esac			
	;;

esac

exit $result
